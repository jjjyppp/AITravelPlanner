// src/components/SpeechRecognition.jsx
import React, { useState, useRef, useEffect } from 'react';
import XunfeiSpeechService from '../services/xunfeiSpeechService';
import './SpeechRecognition.css';

const SpeechRecognition = ({ 
  onResult, 
  onError, 
  placeholder = "ç‚¹å‡»å¼€å§‹è¯­éŸ³è¾“å…¥...",
  disabled = false,
  className = ""
}) => {
  const [isListening, setIsListening] = useState(false);
  const [errorMessage, setErrorMessage] = useState('');
  const [audioLevel, setAudioLevel] = useState(0);
  const [isTestingMic, setIsTestingMic] = useState(false);
  const [testMicLevel, setTestMicLevel] = useState(0);
  const [result, setResult] = useState('');
  
  const audioContextRef = useRef(null);
  const isStoppingRef = useRef(false);
  const audioProcessorRef = useRef(null);
  const audioSourceRef = useRef(null);
  const streamRef = useRef(null);
  const resultRef = useRef('');
  const audioBufferRef = useRef([]); // éŸ³é¢‘ç¼“å†²åŒº
  const sendTimerRef = useRef(null); // å‘é€å®šæ—¶å™¨
  const silenceTimerRef = useRef(null); // é™éŸ³æ£€æµ‹å®šæ—¶å™¨
  const lastAudioTimeRef = useRef(0); // ä¸Šæ¬¡æ£€æµ‹åˆ°éŸ³é¢‘çš„æ—¶é—´
  
  const speechServiceRef = useRef(new XunfeiSpeechService());

  // ==========================================
  // è®¯é£APIå‡­è¯
  // ==========================================
  const XF_CONFIG = {
    appId: 'f7ae70c1',
    apiKey: '557206bc97aa567d51c22e37e2faa9b2',
    apiSecret: 'NTdmOGIyNjU3MWNkYzQzOGNmNWFjZGNi'
  };

  // è®¾ç½®è¯­éŸ³æœåŠ¡å›è°ƒ
  useEffect(() => {
    speechServiceRef.current.setCallbacks({
      onMessage: (data) => {
        try {
          // æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸå¸§
          if (XunfeiSpeechService.isEndFrame(data)) {
            stopListening();
            onResult && onResult(resultRef.current, true);
            return;
          }
          
          // å¤„ç†è¯†åˆ«ç»“æœ
          const recognitionResult = XunfeiSpeechService.processRecognitionResult(data);
          if (recognitionResult) {
            resultRef.current = recognitionResult;
            setResult(recognitionResult);
            onResult && onResult(recognitionResult, true);
          }
        } catch (err) {
          console.error('è®¯é£è¯†åˆ«é”™è¯¯:', err.message);
          const errorMsg = 'è¯­éŸ³è¯†åˆ«é”™è¯¯: ' + err.message;
          setErrorMessage(errorMsg);
          onError && onError(new Error(errorMsg));
        }
      },
      onError: (error) => {
        console.error('WebSocketé”™è¯¯:', error);
        const errorMsg = 'è¿æ¥è®¯é£æœåŠ¡å¤±è´¥: ' + (error.message || 'æœªçŸ¥é”™è¯¯');
        setErrorMessage(errorMsg);
        onError && onError(new Error(errorMsg));
      },
      onClose: () => {
        // åªåœ¨éä¸»åŠ¨åœæ­¢æ—¶è°ƒç”¨onStop
        if (!isStoppingRef.current) {
          stopListening();
        }
      }
    });

    // ç»„ä»¶å¸è½½æ—¶æ¸…ç†
    return () => {
      stopListening();
    };
  }, [onResult, onError]);

  // å¼€å§‹å½•éŸ³
  const startListening = async () => {
    if (disabled) {
      return;
    }

    try {
      setErrorMessage('');
      setResult('');
      resultRef.current = '';
      isStoppingRef.current = false;
      audioBufferRef.current = [];
      lastAudioTimeRef.current = Date.now();
      
      // æ¸…ç†ä¹‹å‰çš„å®šæ—¶å™¨
      if (sendTimerRef.current) {
        clearInterval(sendTimerRef.current);
        sendTimerRef.current = null;
      }
      
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
      
      // å…ˆè·å–ç”¨æˆ·åª’ä½“æƒé™
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      streamRef.current = stream;
      
      // åˆ›å»ºéŸ³é¢‘ä¸Šä¸‹æ–‡
      audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: 16000
      });
      
      // ç¡®ä¿éŸ³é¢‘ä¸Šä¸‹æ–‡æ­£åœ¨è¿è¡Œ
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
      }
      
      // åˆ›å»ºéŸ³é¢‘æºèŠ‚ç‚¹
      audioSourceRef.current = audioContextRef.current.createMediaStreamSource(stream);
      
      // åˆ›å»ºScriptProcessorèŠ‚ç‚¹
      audioProcessorRef.current = audioContextRef.current.createScriptProcessor(4096, 1, 1);
      
      // è®¾ç½®éŸ³é¢‘å¤„ç†å›è°ƒ
      audioProcessorRef.current.onaudioprocess = (e) => {
        if (!isStoppingRef.current && speechServiceRef.current.isConnected) {
          const inputData = e.inputBuffer.getChannelData(0);
          
          // æ£€æŸ¥éŸ³é¢‘è¾“å…¥å¼ºåº¦
          let maxAmplitude = 0;
          for (let i = 0; i < inputData.length; i++) {
            const amplitude = Math.abs(inputData[i]);
            maxAmplitude = Math.max(maxAmplitude, amplitude);
          }
          
          // æ›´æ–°éŸ³é¢‘å¼ºåº¦æ˜¾ç¤º
          setAudioLevel(maxAmplitude * 100);
          
          // æ£€æŸ¥æ˜¯å¦æœ‰éŸ³é¢‘è¾“å…¥
          if (maxAmplitude > 0.01) { // æœ‰éŸ³é¢‘è¾“å…¥
            lastAudioTimeRef.current = Date.now();
            
            // æ¸…é™¤é™éŸ³å®šæ—¶å™¨
            if (silenceTimerRef.current) {
              clearTimeout(silenceTimerRef.current);
              silenceTimerRef.current = null;
            }
          }
          
          // è½¬æ¢ä¸º16ä½æ•´æ•°æ•°ç»„
          const pcmData = new Int16Array(inputData.length);
          for (let i = 0; i < inputData.length; i++) {
            const sample = Math.max(-1, Math.min(1, inputData[i]));
            pcmData[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
          }
          
          // å°†éŸ³é¢‘æ•°æ®æ·»åŠ åˆ°ç¼“å†²åŒº
          audioBufferRef.current.push(...pcmData);
        }
      };
      
      // è¿æ¥éŸ³é¢‘èŠ‚ç‚¹
      audioSourceRef.current.connect(audioProcessorRef.current);
      audioProcessorRef.current.connect(audioContextRef.current.destination);
      
      // è¿æ¥è®¯é£æœåŠ¡
      await speechServiceRef.current.connect(XF_CONFIG);
      
      // å‘é€ç¬¬ä¸€å¸§
      speechServiceRef.current.sendInitialFrame();
      
      // å¯åŠ¨å®šæ—¶å‘é€éŸ³é¢‘æ•°æ®
      startSendingAudio();
      
      // å¯åŠ¨é™éŸ³æ£€æµ‹
      startSilenceDetection();
      
      setIsListening(true);
    } catch (err) {
      console.error('å¯åŠ¨å½•éŸ³å¤±è´¥:', err);
      const errorMsg = 'å¯åŠ¨å½•éŸ³å¤±è´¥: ' + (err.message || 'æœªçŸ¥é”™è¯¯');
      setErrorMessage(errorMsg);
      onError && onError(new Error(errorMsg));
      stopListening();
    }
  };

  // å¯åŠ¨å®šæ—¶å‘é€éŸ³é¢‘æ•°æ®
  const startSendingAudio = () => {
    sendTimerRef.current = setInterval(() => {
      if (audioBufferRef.current.length >= 1280) { // 40msæ•°æ®
        // å–å‡ºä¸€å¸§æ•°æ®
        const frameData = audioBufferRef.current.splice(0, 1280);
        
        // è½¬æ¢ä¸ºå­—èŠ‚æ•°ç»„
        const bytes = new Uint8Array(new Int16Array(frameData).buffer);
        
        // ç¼–ç ä¸ºbase64
        let binary = '';
        for (let i = 0; i < bytes.byteLength; i++) {
          binary += String.fromCharCode(bytes[i]);
        }
        const base64Data = btoa(binary);
        
        // å‘é€éŸ³é¢‘æ•°æ®
        speechServiceRef.current.sendAudioData(base64Data);
      }
    }, 40); // æ¯40mså‘é€ä¸€æ¬¡
  };

  // å¯åŠ¨é™éŸ³æ£€æµ‹
  const startSilenceDetection = () => {
    // æ¯ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦é•¿æ—¶é—´æ²¡æœ‰éŸ³é¢‘è¾“å…¥
    silenceTimerRef.current = setInterval(() => {
      const now = Date.now();
      const timeSinceLastAudio = now - lastAudioTimeRef.current;
      
      // å¦‚æœè¶…è¿‡5ç§’æ²¡æœ‰éŸ³é¢‘è¾“å…¥ï¼Œåˆ™åœæ­¢å½•éŸ³
      if (timeSinceLastAudio > 5000 && isListening) {
        stopListening();
      }
    }, 1000);
  };

  // åœæ­¢å½•éŸ³
  const stopListening = () => {
    isStoppingRef.current = true;
    
    // æ¸…ç†å®šæ—¶å™¨
    if (sendTimerRef.current) {
      clearInterval(sendTimerRef.current);
      sendTimerRef.current = null;
    }
    
    if (silenceTimerRef.current) {
      clearInterval(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    
    // å‘é€å‰©ä½™çš„éŸ³é¢‘æ•°æ®
    while (audioBufferRef.current.length >= 1280) {
      const frameData = audioBufferRef.current.splice(0, 1280);
      const bytes = new Uint8Array(new Int16Array(frameData).buffer);
      let binary = '';
      for (let i = 0; i < bytes.byteLength; i++) {
        binary += String.fromCharCode(bytes[i]);
      }
      const base64Data = btoa(binary);
      speechServiceRef.current.sendAudioData(base64Data);
    }
    
    // å‘é€æœ€åä¸è¶³ä¸€å¸§çš„æ•°æ®
    if (audioBufferRef.current.length > 0) {
      const remainingData = new Int16Array(audioBufferRef.current);
      const bytes = new Uint8Array(remainingData.buffer);
      let binary = '';
      for (let i = 0; i < bytes.byteLength; i++) {
        binary += String.fromCharCode(bytes[i]);
      }
      const base64Data = btoa(binary);
      speechServiceRef.current.sendAudioData(base64Data);
      audioBufferRef.current = [];
    }
    
    // å‘é€ç»“æŸå¸§
    speechServiceRef.current.sendEndFrame();
    
    // æ–­å¼€éŸ³é¢‘å¤„ç†å™¨è¿æ¥
    if (audioProcessorRef.current) {
      audioProcessorRef.current.disconnect();
      audioProcessorRef.current = null;
    }
    
    // æ–­å¼€éŸ³é¢‘æºè¿æ¥
    if (audioSourceRef.current) {
      audioSourceRef.current.disconnect();
      audioSourceRef.current = null;
    }
    
    // åœæ­¢éŸ³é¢‘æµ
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => {
        track.stop();
      });
      streamRef.current = null;
    }
    
    // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    
    setIsListening(false);
  };

  // æµ‹è¯•éº¦å…‹é£
  const testMicrophone = async () => {
    try {
      setIsTestingMic(true);
      setTestMicLevel(0);
      
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          sampleRate: 16000,
          channelCount: 1
        } 
      });
      
      const audioContext = new (window.AudioContext || window.webkitAudioContext)({
        sampleRate: 16000
      });
      
      const source = audioContext.createMediaStreamSource(stream);
      const processor = audioContext.createScriptProcessor(4096, 1, 1);
      
      processor.onaudioprocess = (e) => {
        const inputData = e.inputBuffer.getChannelData(0);
        let maxAmplitude = 0;
        for (let i = 0; i < inputData.length; i++) {
          maxAmplitude = Math.max(maxAmplitude, Math.abs(inputData[i]));
        }
        setTestMicLevel(maxAmplitude * 100);
      };
      
      source.connect(processor);
      processor.connect(audioContext.destination);
      
      // 5ç§’ååœæ­¢æµ‹è¯•
      setTimeout(() => {
        processor.disconnect();
        source.disconnect();
        stream.getTracks().forEach(track => track.stop());
        audioContext.close();
        setIsTestingMic(false);
        setTestMicLevel(0);
      }, 5000);
      
    } catch (err) {
      console.error('éº¦å…‹é£æµ‹è¯•å¤±è´¥:', err);
      setErrorMessage('éº¦å…‹é£æµ‹è¯•å¤±è´¥: ' + err.message);
      setIsTestingMic(false);
      setTestMicLevel(0);
    }
  };

  // åˆ‡æ¢å½•éŸ³çŠ¶æ€
  const toggleListening = () => {
    if (isListening) {
      stopListening();
    } else {
      startListening();
    }
  };

  // æ¸…ç©ºç»“æœ
  const clearResult = () => {
    resultRef.current = '';
    setResult('');
    setErrorMessage('');
  };

  return (
    <div className={`speech-recognition ${className}`}>
      <div className="speech-controls">
        <button
          type="button"
          className={`speech-button ${isListening ? 'listening' : ''} ${disabled ? 'disabled' : ''}`}
          onClick={toggleListening}
          disabled={disabled}
        >
          {isListening ? 'â¹ï¸ åœæ­¢å½•éŸ³' : 'ğŸ¤ è¯­éŸ³è¾“å…¥(è®¯é£)'}
        </button>
        <button
          type="button"
          className="test-microphone-button"
          onClick={testMicrophone}
          disabled={isListening}
        >
          ğŸ”§ æµ‹è¯•éº¦å…‹é£
        </button>
        
        {result && (
          <button
            type="button"
            className="clear-button"
            onClick={clearResult}
            title="æ¸…ç©ºç»“æœ"
          >
            ğŸ—‘ï¸
          </button>
        )}
      </div>
      
      {isListening && (
        <div className="audio-level-indicator">
          <div className="audio-level-label">
            ğŸ¤ éŸ³é‡: {audioLevel.toFixed(0)}% 
            {audioLevel > 5 ? ' ğŸ”Š' : audioLevel > 1 ? ' ğŸ”‰' : ' ğŸ”ˆ'}
          </div>
          <div className="audio-level-bar">
            <div 
              className="audio-level-fill" 
              style={{ 
                width: `${Math.min(audioLevel * 3, 100)}%`,
                backgroundColor: audioLevel > 10 ? '#4CAF50' : audioLevel > 5 ? '#FFC107' : '#FF5722'
              }}
            ></div>
          </div>
        </div>
      )}
      
      {isTestingMic && (
        <div className="audio-level-indicator test-mic-indicator">
          <div className="audio-level-label">
            ğŸ”§ æµ‹è¯•éº¦å…‹é£: {testMicLevel.toFixed(0)}% 
            {testMicLevel > 5 ? ' ğŸ”Š' : testMicLevel > 1 ? ' ğŸ”‰' : ' ğŸ”ˆ'}
          </div>
          <div className="audio-level-bar">
            <div 
              className="audio-level-fill" 
              style={{ 
                width: `${Math.min(testMicLevel * 3, 100)}%`,
                backgroundColor: testMicLevel > 10 ? '#4CAF50' : testMicLevel > 5 ? '#FFC107' : '#FF5722'
              }}
            ></div>
          </div>
        </div>
      )}
      
      {errorMessage && (
        <div className="speech-error">
          {errorMessage}
          <button 
            type="button" 
            className="clear-error-btn"
            onClick={() => setErrorMessage('')}
          >
            âœ•
          </button>
        </div>
      )}
      
      {result && (
        <div className="speech-result">
          <div className="result-label">è¯†åˆ«ç»“æœï¼š</div>
          <div className="result-text">{result}</div>
        </div>
      )}
      
      {isListening && (
        <div className="voice-recording-indicator">
          æ­£åœ¨å½•éŸ³...è¯·ç»§ç»­è¯´è¯ (æ£€æµ‹åˆ°é™éŸ³5ç§’åè‡ªåŠ¨åœæ­¢)
        </div>
      )}
      
      <div className="speech-tips">
        <p>ğŸ’¡ ä½¿ç”¨æç¤ºï¼š</p>
        <ul>
          <li>è¯·ç¡®ä¿éº¦å…‹é£æƒé™å·²å¼€å¯</li>
          <li>å»ºè®®åœ¨å®‰é™ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œæ•ˆæœæ›´ä½³</li>
          <li>æ”¯æŒä¸­æ–‡æ™®é€šè¯è¯†åˆ«</li>
          <li>æ£€æµ‹åˆ°é™éŸ³5ç§’åè‡ªåŠ¨åœæ­¢å½•éŸ³</li>
        </ul>
      </div>
    </div>
  );
};

export default SpeechRecognition;